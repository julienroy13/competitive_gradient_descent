# competitive_gradient_descent
Re-implementation of Competitive Gradient Descent


### Implementation

* Implement CGD
  * Should we try in Pytorch or JAX ?
    * Investigate how to do (1) forward differentiation in pytorch and (2) second order derivatives in pytorch
    * Try-out JAX a bit (is it complicated to use/learn?)
* Implement GDA (easy)
* Implement other baselines (which ones?)

* Implement a GAN and training pipeline on MNIST

### Experiments

* Experiment of Figure 2
* Experiment of Figure 3
* Train GAN on MNIST (easy dataset) and compare performance and robustness with different optimizers

* Non zero-sum games (similar to Figure 2) (maybe just discussions in the report?)

### Poster

* Overview of the paper
  *
  *
  *
* Experiment results
  *
  *
  *
